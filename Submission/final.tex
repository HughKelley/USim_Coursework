% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage[backend=biber, style=authoryear]{biblatex}
\addbibresource{bibliography.bib}
%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

%\usepackage{graphicx} % support the \includegraphics command and options

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{graphicx}
\usepackage{csvsimple}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{\vspace{-3.0cm}Urban Simulation Assessment}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Part 1}

% to combine each part into one overall repo for the submission. ended up just adding all the files to a new repo bc it didn't look like the procedure was compatible with github

% in hindsight, I probably could have forked the Main repo, added the files from another part and merged twice. Would that have been better? 
% https://saintgimp.org/2013/01/22/merging-two-git-repositories-into-one-repository-without-losing-file-history/

\subsection{Introduction}

This is an analysis of resilience in the London tube network. The analysis uses a recursive function for node removal to calculate the marginal effect of a node's removal, pseudo-code for the function can be found in Appendix 1. Below, criteria for node removal and effect evaluation are discussed below. 


\subsection{Impact Evaluation}

Because the node removal criteria was decided in the context of the network effect metric, the network effect metric will be discussed first. 

\textbf{Calculate community metric for network to justify focus on distance?}
\textit{from igraph: cluster edge betweenness, cluster fast\_greedy, cluster\_label\_prop, cluster\_leading\_eigen, cluster louvain, cluster optimal, cluster spinglass, cluster walktrap.}

Because London's tube doesn't demonstrate strong sub clusters,breaking the network into isolates was investigated but not pursued. Instead, the focus will be on forcing tube users to travel further for longer on their journeys. 

This is investigated using shortest path and shortest topological path. Given the spatial nature of the london tubes, where edge attributes represent actual distances, true shortest path might be an attractive option. In the context of the London tube though, total time and effort are more important than total distance though. Trains can travel longer distances fairly rapidly while travelling through a high number of stations increases time dramatically because of the need to stop. Further, it is assumed that traveling through a higher number of stops implies a higher number of time and effort consuming train changes to switch. Thus by using geodesic, what is being maximized is the increase in stoppage time, and line change time for travelers in the network.

The statistics calculated to understand the effect of node removal include: \% of connected nodes in the graph, average of the node specific statistics above, 

The igraph package's mean\_distance\(\) function was used to compute the average shortest path between nodes in the network. The unconnected parameter was used to specify that nodes that were not connected to the largest cluster were counted as 1 + the longest possible geodesic, the actual longest geodesic is much less than this. This demonstrates the incompatibility of different network measures. There isn't really a way to compare the effect of a longer trip with the effect of removing a trip possibility entirely. To do that we would have to include alternate modes of transport like the bus network.  The actual longest geo

\subsection{Node Removal Criteria}

The function was run for measures that include, degree, betweeness, topological betweenness, closeness, topological closeness and eigenvector centrality. The correlations for these values across stations can be reviewed in figure 1. It was noted that correlations between weighted and topological measures were high, indicating that the distances between tube stations are fairly consistent so that the number of stations between two stations is a decent approximation of the distance. The correlation of measures betweenness and degree is also fairly high, indicating that tube stations at the middle of a line, with higher betweenness, also tend to have multiple lines, high degree. Correlations between eigenvector centrality and the other measures was very low, indicating that this does not give the same information as other metrics. Lastly, correlation between weighted and topological eigenvector centrality was 0 \textbf{indicating a problem with how they were calculated!}. 

In order to maximize the increase in travel time measured by the average length of geodesic paths, betweenness will be used to order node removals. This measure is the number of shortest paths between nodes that travel through a given node. Deleting the node with highest betweenness will force the highest number of trips to use an alternate, ostensibly longer, path through other stations. 

One note about this process, deleting nodes in some places creates isolates. In this investigation \textbf{this was not an issue I hope because for the first 10 or so nodes by betweenness, they were not the only node connecting any other node to the network.}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{Corr_tube_graph_node_stats}
\caption{Correlation between station/node metrics}
\end{figure}


\begin{center}
\csvautotabular{betfalse.csv}
\end{center}

% gotta reformat this probably asking about it on tex stack exchange

\begin{tabular}{|c|c|c|c|}\hline %
  \multicolumn{4}{|c|}{\bfseries Betweenness unconn is false}  \\ \hline
 & Node Deleted & Increase in Avg Geodesic & Components
  \csvreader[head to column names]{betfalse.csv}{} %
 {\\\index & \NodeDeleted & \IncreaseGeodesic & \Components} %
 \\\hline
 \end{tabular}
 


\begin{center}
\csvautotabular{bettrue.csv}
\end{center}

\begin{center}
\csvautotabular{eigtrue.csv}
\end{center}

\begin{center}
\csvautotabular{eigfalse.csv}
\end{center}




\subsection{Analysis}

When Kings Cross is deleted, it creates a new unconnected component out of the 11 stations on the north east end of the Picadilly line. In igraph, the two ways to handle this for the mean\_distance() function are either to exclude distances between those unconnected nodes and the rest of the network or to assume that the distance is one greater than the longest possible geodesic in the network, that is ,the number of nodes on the network. Data is included for both options. 

Looking at the effect data it seems reasonable to say that betweenness did a better job than eigenvector centrality of prioritizing nodes to remove. Using betweenness created more isolates. It's difficult to judge which method lengthened average shortest path the most because of the options for dealing with disconnected networks. This will be discussed in the conclusion. 



\subsection{Conclusions}

To improve this work, it would be good to add data about transportation networks besides the underground. In particular information about bus routes connected nodes would be useful because it would allow for a better estimate of average shortest path when subway stations become disconnected as the shortest path could then go through a bus route instead. Similarly, it would be good to include more granular data about where a rider would have to change trains. The current network assumes there's no cost to switch trains relative to staying on the same train passing through a station. Anyone who has walked from the Picadilly line to the Northern line at Kings Cross knows that there is a big difference. 

An improvement to the data would be to use travel time data instead of using distance as an approximation. 

Lastly, it would be interesting to build an igraph function that can compute average shortest path using edge weights since the current function cannot. This could confirm or reject the thought that tube stations are spaced fairly regularly based on the high correlation between weighted and topological centrality measures. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak

\section{Part 2}

\subsection{The Models}

\subsubsection{Unconstrained}

The model is constrained to the total flows of the system but flows out of an origin and into a destination can be any value between 0 and total system flows. 

This is useful for studying the change in connectivity between regions, for instance if a new transportation link was built. In particular, it is useful for studying long term effects of a change where residence and employment are more flexible. 

\subsubsection{Production}

The direction of flows can change but the total flows from each origin will remain constant. This is useful for studying the effect of a new employment or consumption location that changes the destinations of people going to work or to spend money. In terms of the matrix, it implies that the sums of the rows of the matrix are constant. 

\subsubsection{Attraction Constrained}

The source of flows into a region can change but the total flows into a region will remain constant. That is, any reduction in flows into a destination from another region will be fully replaced by flows into the destination from another region.  This could be used to study a new housing development that pulls people into residence in a different part of an area or a natural disaster that forces residents out of an area. Employers outside the area still need workers but will not be able to draw them from the same places after a housing change or natural disaster. In terms of the matrix the sums of the columns are held constant. Additionally, it can be used to study the effect of a specific change to employment where the model can be constrained to the values that result from that change. 

\subsubsection{Doubly Constrained,}

Doubly constrained models could be used to test the short term effects of a change to transportation networks given that homes and businesses won't relocate but flexible behavior patterns like shopping could change almost immediately due to the change in accessibility or travel times between locations. In this model, the sums of both the columns and rows are held constant.

\subsection{The Parameters}



Sensitivities to origin attributes, destination attributes, and linkage attributes

\subsection{A Scenario}

The scenario used for this assessment will be: What if teleportation was invented and dramatically reduced travel times but could only be used in the outer boroughs because of the need to avoid teleporting through tall buildings. Thus origin and destination attributes remain constant but the travel costs change dramatically in the most peripheral boroughs: Hilingdon, Harrow, Barnet, Enfield, Waltham forest, Redbridge, Havering, Bexley, Bromley, Croydon, Sutton, Kingston, Richmond, Hounslow. 

This will be investigated using a doubly constrained model to estimate the short term effects where residences and businesses cannot relocate and a total constrained model to see the long term effects on business and residence locations as a result of the incredible new discovery. 


I selected a subset of the london Boroughs that included the main business areas, WEstminister, the city, and Camden, as well as the outermost boroughs

idea: pick out the main business areas, city, westminister, camden 

and 

Then do a production constrained model about what would change if salaries in one of the urban boroughs went up, or if a new transit line went in. 

\textit{select a scenario and explore the consequences of varying model parameters and inputs on interaction flows and the origin/destination estimates}

One thing noticed is that as the number of constraints goes up the $R^2$ goes up. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Part 3}

\subsection{Overview}

\textit{define, compare and contrast Cellular Automata and Agent Based models}
Compare CA and ABM

While Cellular automata models are often viewed as separate modeling techniques, a more modern view of these methods considers them to be cousins or related in the sense that all CA are a subset of ABM. 

Expanding a CA model usually results in the creation of an ABM.  Cellular automata models are defined by a set of homogeneous cells that interact only with each other according to a defined set of input/output functions, e.g., if a cell with value 1 is surrounded by other cells f value 1 it's value becomes 0. The models can be extended to "n" states but all cells should be capable of reaching all n states to maintain the homogeneity of the cells. This can be contrasted with ABM where cells can have infinite heterogeneity and future states can be functions of the "environment" other cells or agents" and the cells own state. 

The simplicity of CA models make them very useful for studying mathematical processes whereas Agent Based Modeling flexibility make them useful for modeling more complex "real world" phenomena and make them more accessible to non-technical audiences. Often the value of ABM comes from the ability to conduct parameter sweeps, to study combination rules that apply to multiple conceptual processes with different parameter values. The value of cellular automata models tends to be focused on the effect of initialization states on the long term outcomes and equillibria of the model e.g. for the same model, outcomes are a function of the rule set and initialization values, where as agent based models tend to be calculated for a large number of initialization values in order to study the effect of model dynamics independent of initialization values that may not accurately reflect the real world. 

\textit{vary model parameters to construct 3 scenarios, describe them, find time required to reach steady state, minimum runs required for statistically meaningful results.}


\subsection{Scenarios}

This work will investigate three possible infection scenarios. The first is a baseline scenario used to explore a moderately recoverable disease to which immunity is low, for instance the seasonal flu without a vaccination program. The second scenario looks at the same disease with a vaccination program. The third looks at a disease like the common cold which is more recoverable but where immunity is close to non-existent. 

All three scenarios will use a population of 1000 and infect 100 people initially. 

\subsubsection{Scenario 1}
%%%%%%%%%%%%%%%%%%%%%%
% describe 

The first scenario, looking at a flu type disease with no vaccine available uses 50\% chance of immunity, based on the idea that it's fairly common for someone with the flu to interaction with someone else without infecting them and a 10\% chance of recovery based on the guess that flu symptoms last between 3 and 9 days for the majority of the population.

% time required to reach steady state

Looking at Figure 2, which plots the percentage of turtles infected over 150 steps of the model for 20 trials, a steady state becomes fairly clear. While this is not a perfect equilibrium, compared to the rapid increase in the first 20 ticks, the model becomes fairly stationary.  This, future runs of this model can be limited to 40 ticks with reasonably confidence that the model will have found an equilibrium. 

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{scen_1_steady_state}
\caption{\% of Population Infected Over Time \\ Scenario 1}
\end{figure}


% minimum runs for statistically meaningful results

The average steady state percentage of turtles infected at 40 ticks was 38.02. The standard deviation was 1.869, which indicates a margin of error at 95\% confidence of $\pm$ 0.831. Thus one can expect that 95\% of runs with these parameters will be within 1\% of 38\% infection at steady state. This was calculated for 20 trials. At 10 trials the mean percent of turtles infected 37.96\% $\pm0.27$ margin of error and mean of time infected was 23.822 $\pm$ 0.1522. For 5 trials the mean percent infected was $37.46\pm0.296$ and for total time the mean was 23.91 $\pm$ 0.138. Thus it is clear that this model is simple enough that for the parameters specified few trials are needed to be confident in the stability of results obtained given the low variation in outcome. This is demonstrated between trial sets with the outcomes of the 20, 10, and 5 trial runs very similar to each other. 

The total infection time per non-immune turtle was also recorded. The 20 trial mean at tick 40 was 23.51 and standard deviation was 0.913. This gives a 95\% margin of confidence of $\pm0.40$. 

%redo at tick 50. 

For both of these variables, 20 trials gives a very narrow band of expected outcomes for a 95\% confidence interval. 

% conclusions. 

Thus the model indicates that for these parameters, a fairly high number of turtles will contract the infection and the infection will be passed around continually rather than dying out at some point in the future. Considering the margin of errors for the results, if the model accurately reflects a real world scenario, we can have a good idea of how that scenario would play out. 




\subsubsection{Scenario 2}
%%%%%%%%%%%%%%%%%%%%
% describe

Scenario 2 looks at the flu in a society with high but imperfect immunization rates. In the US this is seen in cases where infants and the elderly cannot receive the vaccine and some people elect not to receive it or forget. This is accomplished by using the same recovery rate, 10\% but increasing immunity probability to 80\%. 


% time required to reach steady state

In Figure 3 the distribution over time of the percentage of turtles infected in the scenario is illustrated. It can be seen that a steady state is more difficult ot identify. One consideration is that the average percentage is lower than in scenario 1 because 80\% of the population is immune. A second is that the lower mean scales the y-axis of the chart down from 0-14 instead of 0-45 in scenario 1. Thus while the standard deviation looks higher it is essentially the same, 1.869 in scenario 1 and 1.853 in scenario 2. 

Because a steady state is less certain looking at the chart a t-test can be used to confirm that the mean of the set of trials is not changing between steps. At step 50 the mean percent infected was 5.92 with st. deviation 1.853. At step 100 these were 6.90 and 1.99. At step 250 7.935 and 1.696. At step 400 8.075 and 1.502. At step 500 7.956 and 2.113. 

The p-value for a t-test with the null hypothesis that the means at steps 50 and 100 are different was 0.116. Indicating a 12\% chance that they are different and that a stead state has not been reached with 95\% confidence. The same test for the means at tick 250 and tick 500 had a p-value of 0.97. The test for steps 400 and 500 had a p-value of 0.84. Thus it is difficult to say with confidence that a steady state exists for this model as the effort to reject the null hypothesis that the means are different begins to resemble simply getting lucky enough to choose ticks with means that are very similar to each other by coincidence. 

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{20-runs-scenario-2-steady-state}
\caption{\% of Population Infected Over Time \\ Scenario 2}
\end{figure}


% minimum runs for statistically meaningful results

at tick 250, the mean percentage of turtles infected was $7.93 \pm 0.332$ again demonstrating that the variability in results for this scenario is very similar to those of scenario 1 but that in light of the results of a t-test for a steady state, the true margin of error is likely to be higher since the process may not be stationary at tick 250, that is to say, the true mean may change across ticks.  Finally, smaller sets of trials exhibited similar results to those calculated in scenario 1 showing that the change in parameters affected the value of the results but not their variability. 

\subsubsection{Scenario 3}

% describe

This scenario will investigate how the common cold compares to the flu by using a higher rate of recovery 25\% and lower rate of immunity 5\% since there is no vaccine for the common cold and many members of society continue their daily routines after contracting it exposing relatively more people than the flu, which most people suffer at home. 

% time required to reach steady state

Figure 4 illustrates the percentage of turtles infected over time. Here a steady state seems more obvious. At 50 ticks the mean was 66.19 and st. deviation was 1.597. At 100 ticks this was 65.84 and 1.648. The 0.4994 p-value for a two tailed t-test for difference in means indicates only a 5\% probability of the null hypothesis that the means are different, just on the border of an acceptable level of confidence but basically acceptable.\footnote{If I was testing arbitrary ticks like 53 and 107 against each other, this level of confidence might be more concerning.}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{20-runs-scenario-3-steady-state}
\caption{\% of Population Infected Over Time \\ Scenario 3}
\end{figure}

% minimum runs for statistically meaningful results

At 50 ticks the mean percentage of turtles infected was $66.19  \pm 0.313$ . Thus the low variability of outcomes for different outcome values as a result of different parameter values continues. Checking sets of five and ten trials revealed that very few trials were necessary to get a fairly tight 95\% confidence interval for the model outcomes. 


\subsection{Conclusion}

Comparing the three scenarios, The unvaccinated flu, scenario 1, moderate immunity (50\%) and moderate recovery rates (10\%) resulted in a 38\% average infection rate while the vaccinated flu 80\% immunity and 10\% recovery resulted in 7.93\% of turtles infected on average. The common cold, 5\% immunity, 25\% chance of recovery gave 66\% of turtles infected. At tick 50, Scenario 1 turtles that were not immune had spent $23.6 \pm 0.12$ ticks sick on average while this was $7.54 \pm 0.45$ in scenario 2 and $24.14 \pm 0.05$ in scenario 3. 

Thus it is clear that higher rates of vaccination in scenario 2 lead to lower amounts of sicktime for turtles without the vaccine. At a high level it can then be said that this model replicates the empirical idea of ``herd immunity'' where general immunity of a group protects non-immune members of the group. 

To continue investigating this, the model could be explored to find whether there is a constant increase in sick time per non-immune turtle or whether as population immunity decreases sick time accelerates. It would also be useful to add in a parameter for the contagiousness of the infection, the radius within which turtles affect each other. Lastly, adding in death or post-infection immunity would be an interesting extension. 


1709 words excluding headings, figures, and references. \\

Do I need to cite anything? 

\nocite{*}


\printbibliography


\end{document}
